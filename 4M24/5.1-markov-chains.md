### Content: 
- The fundamental concepts of Markov chains and the Markov property.
- Detailed computations using a discrete example.
- The concept of invariant distributions and how to verify them.
- The conditions necessary for a Markov chain to converge to its invariant distribution.
- The connection between Markov chains and MCMC methods.

## Introduction
Markov chains are fundamental tools in probability theory and stochastic processes. They model systems that undergo transitions from one state to another in a chain-like manner, where the probability of moving to the next state depends solely on the present state and not on the sequence of events that preceded it. This property is known as the Markov property.

## The Markov Property
At the core of Markov chains is the Markov property, which stipulates that the future state of a process depends only on its present state, not on its past history.

Mathematically, for a sequence of random variables $\{\theta^{(i)}\}$, the Markov property is expressed as:
$$
P(\theta^{(i)} \mid \theta^{(i-1)}, \theta^{(i-2)}, \dots, \theta^{(1)}) = T(\theta^{(i)} \mid \theta^{(i-1)})
$$

Here:
- $P(\theta^{(i)} \mid \theta^{(i-1)}, \dots, \theta^{(1)})$ is the conditional probability of being in state $\theta^{(i)}$ given all previous states.
- $T(\theta^{(i)} \mid \theta^{(i-1)})$ is the transition probability from state $\theta^{(i-1)}$ to state $\theta^{(i)}$.

## Homogeneous Markov Chains
A Markov chain is homogeneous if its transition probabilities do not change over time. This means that $T(\theta^{(i)} \mid \theta^{(i-1)})$ is the same for all time steps $i$. In mathematical terms:
$$
T(\theta^{(i)} \mid \theta^{(i-1)}) = T(\theta^{(1)} \mid \theta^{(0)})
$$

for all $i$.

Homogeneity simplifies the analysis because the same transition operator $T$ applies at each step.

## Transition Probabilities
For the transition probabilities to be valid, they must satisfy normalization conditions:

### Discrete State Space:
For all $\theta^{(i-1)}$:
$$
\sum_{\theta^{(i)}} T(\theta^{(i)} \mid \theta^{(i-1)}) = 1
$$
This ensures that the total probability of transitioning from state $\theta^{(i-1)}$ to all possible next states sums to 1.

### Continuous State Space:
For all $\theta^{(i-1)}$:
$$
\int_{\theta^{(i)} \in \Theta} T(\theta^{(i)} \mid \theta^{(i-1)}) \, d\theta^{(i)} = 1
$$
Here, $\Theta$ is the state space, and the integral ensures that the total probability over the continuous state space is 1.

## A Discrete Example
Let's explore a concrete example to illustrate these concepts. Consider a Markov chain with three states, and the transition matrix $T$ is given by:

$$
T = \begin{pmatrix}
0 & 1 & 0 \\
0 & 0.1 & 0.9 \\
0.6 & 0.4 & 0
\end{pmatrix}
$$
Here, $T_{ij}$ represents the probability of transitioning from state $i$ to state $j$. Note that each row sums to 1, satisfying the normalization condition.

Suppose the initial probability distribution over the states is:
$$
\pi(\theta^{(1)}) = [0.5, 0.2, 0.3]
$$
## Computing the Next State Distribution
To find the state distribution at the next time step, we multiply the initial distribution by the transition matrix:
$$
\pi(\theta^{(2)}) = \pi(\theta^{(1)}) T
$$

Thus, the state distribution at time 2 is:
$$
\pi(\theta^{(2)}) = [0.18, 0.64, 0.18]
$$

## Computing Future State Distributions
To compute the state distribution at time $i$, we can use:

$$
\pi(\theta^{(i)}) = \pi(\theta^{(1)}) T^{(i-1)}
$$

This involves raising the transition matrix $T$ to the power $(i-1)$ and multiplying it by the initial distribution.

## Computing $T^2$
Let's compute $T^2$ to find $\pi(\theta^{(3)})$:

$$
T^2 = T \times T
$$
Multiplying the matrices:

$$
T^2 = \begin{pmatrix}
0 & 0.1 & 0.9 \\
0.54 & 0.37 & 0.09 \\
0 & 0.64 & 0.36
\end{pmatrix}
$$

This new matrix $T^2$ represents the probabilities of transitioning from one state to another over two steps.

## Higher Powers of $T$
Similarly, we can compute higher powers of $T$:
  
  $$
  T^{16} = \begin{pmatrix}
  0.2174 & 0.4066 & 0.3760 \\
  0.2256 & 0.4085 & 0.3659 \\
  0.2189 & 0.4133 & 0.3678
  \end{pmatrix}
  $$

As we raise $T$ to higher powers, the rows of $T^n$ begin to converge to the same values, indicating that the Markov chain is approaching its steady state.

## Steady-State Distribution

$$
T^{64} = \begin{pmatrix}
0.2213 & 0.4098 & 0.3689 \\
0.2213 & 0.4098 & 0.3689 \\
0.2213 & 0.4098 & 0.3689
\end{pmatrix}
$$

All rows are approximately equal, suggesting that regardless of the initial state, the chain will converge to this distribution.

Therefore, the invariant distribution is:
$$
\pi(\theta) = [0.2213, 0.4098, 0.3689]
$$

This distribution is independent of the initial state distribution $\pi(\theta^{(1)})$.

## Invariant Distribution
An invariant distribution (also known as the stationary distribution) $\pi$ of a Markov chain is a probability distribution that remains unchanged as the chain progresses. It satisfies:
$$
\pi = \pi T
$$

## Verifying the Invariant Distribution
To verify that $\pi(\theta) = [0.2213, 0.4098, 0.3689]$ is indeed an invariant distribution, we can compute $\pi T$:

$$
\pi T = [0.2213, 0.4098, 0.3689] \begin{pmatrix}
0 & 1 & 0 \\
0 & 0.1 & 0.9 \\
0.6 & 0.4 & 0
\end{pmatrix}
$$
Thus, $\pi T \approx \pi$, confirming that $\pi$ is indeed the invariant distribution.

## Significance of the Invariant Distribution
The invariant distribution represents the long-term behavior of the Markov chain. Regardless of the starting state, the chain will converge to this distribution if certain conditions are met.

## Conditions for Convergence to the Invariant Distribution
For a Markov chain to converge to its invariant distribution, the following conditions must be satisfied:

1. **Irreducibility**: A Markov chain is irreducible if it's possible to reach any state from any other state in a finite number of steps, with positive probability.

2. **Aperiodicity**: A Markov chain is aperiodic if it does not get trapped in cycles with a fixed period.

3. **Reversibility and Detailed Balance**: A sufficient condition for the existence of an invariant distribution is if the chain is reversible, meaning it satisfies the detailed balance condition:
   $$
   \pi(\theta^{(i)}) T(\theta^{(i-1)} \mid \theta^{(i)}) = \pi(\theta^{(i-1)}) T(\theta^{(i)} \mid \theta^{(i-1)})
   $$

## Deriving the Invariant Distribution from Detailed Balance
Summing both sides over $\theta^{(i-1)}$:

$$
\sum_{\theta^{(i-1)}} \pi(\theta^{(i)}) T(\theta^{(i-1)} \mid \theta^{(i)}) = \sum_{\theta^{(i-1)}} \pi(\theta^{(i-1)}) T(\theta^{(i)} \mid \theta^{(i-1)})
$$

Since $\sum_{\theta^{(i-1)}} T(\theta^{(i-1)} \mid \theta^{(i)}) = 1$, we get:

$$
\pi(\theta^{(i)}) = \sum_{\theta^{(i-1)}} \pi(\theta^{(i-1)}) T(\theta^{(i)} \mid \theta^{(i-1)})
$$
This equation confirms that $\pi$ is an invariant distribution.

## Summary of Conditions
- **Irreducibility** ensures that the chain can explore the entire state space.
- **Aperiodicity** prevents the chain from getting stuck in cycles.
- **Reversibility** or satisfying detailed balance guarantees the existence of an invariant distribution.

When these conditions are met, the Markov chain will converge to a unique invariant distribution regardless of the initial distribution.

## Markov Chain Monte Carlo (MCMC)
Markov Chain Monte Carlo methods are algorithms used to sample from complex probability distributions. They construct a Markov chain whose invariant distribution is the target distribution we wish to sample from.

### Connection to Markov Chains
In MCMC, we often know the invariant distribution $\pi$ (e.g., a posterior distribution in Bayesian statistics), and our goal is to design a transition operator $T$ such that:

- The chain is irreducible and aperiodic.
- The chain satisfies the detailed balance condition with respect to $\pi$.

By ensuring these conditions, we can simulate the Markov chain and, after a sufficient number of steps (known as the burn-in period), collect samples that approximate the target distribution.

## Conclusion
Markov chains provide a powerful framework for modeling stochastic processes where the future depends only on the present. By understanding the Markov property, transition probabilities, invariant distributions, and the conditions for convergence, we gain valuable insights into the long-term behavior of these processes.

## Further Reading
To deepen your understanding of Markov chains, consider exploring the following topics:

- **Ergodic Theorems**: Understanding conditions under which time averages converge to ensemble averages.
- **Continuous-Time Markov Chains**: Extending the concept to processes where changes can occur at any time point.
- **Hidden Markov Models (HMMs)**: Models where the states are not directly observable but can be inferred through observations.
- **Markov Decision Processes (MDPs)**: Frameworks for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker.
