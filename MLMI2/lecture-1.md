# Lecture 1: Introduction to Speech Recognition

## 1. Introduction to Speech and Automatic Speech Recognition (ASR)

### 1.1 The Speech Signal
Speech is a natural and primary mode of communication for humans, characterized by complex acoustic signals generated by the vocal apparatus. These signals are continuous, non-stationary, and carry rich information about linguistic content, speaker identity, emotion, and more.

### 1.2 The Speech Recognition Problem
Automatic Speech Recognition (ASR) aims to develop systems that can automatically convert spoken language into written text. This involves:
- Processing the speech signal to extract relevant ***features***.
- Modeling the ***relationship*** between these features and linguistic units like phonemes, words, or sentences.
- Handling ***variability*** due to different speakers, accents, speaking styles, and acoustic environments.

### 1.3 Approaches to ASR
- Historically, ASR systems have used statistical models like Hidden Markov Models (HMMs) combined with Gaussian Mixture Models (GMMs) for acoustic modeling and N-gram models for language modeling.
- More recently, deep learning techniques using neural networks have become prevalent, leading to end-to-end ASR systems that directly map speech signals to text.

---

## 2. The Speech Waveform and Its Properties

### 2.1 Nature of the Speech Signal
- **Non-Stationarity:** The speech signal changes over time as articulators move to produce different sounds.
- **Mixed Components:** Contains both periodic (voiced sounds) and random (unvoiced sounds) components.

![[Pasted image 20241120120947.png]]

### 2.2 Time-Domain Characteristics
- **Amplitude Variation:** Reflects the energy of speech sounds.
- **Waveform Shape:** Periodic waveforms correspond to voiced sounds, while aperiodic waveforms correspond to unvoiced sounds or noise.

--- 
## 3. Human Speech Production Mechanism

### 3.1 Anatomy of the Vocal Tract
- **Lungs:** Provide airflow by pushing air through the larynx.
- **Larynx and Vocal Folds:** The vocal folds (cords) can vibrate to produce voiced sounds.
- **Vocal Tract:** A variable-shaped acoustic tube formed by the throat, mouth, and nasal passages.
- **Articulators:** Tongue, lips, teeth, and jaw shape the vocal tract to produce different sounds.
![[Pasted image 20241120121123.png]]
### 3.2 Excitation Sources
- **Voiced Sounds:**
  - Produced when the vocal folds vibrate.
  - Result in a quasi-periodic waveform at the *pitch frequency*.
  - Examples: vowels like "ee" in "feel," consonants like "m" in "man."
![[Pasted image 20241120121545.png]]
- **Fricatives:**
  - Produced by creating a constriction in the vocal tract and forcing air through it.
  - Result in a random, noise-like waveform.
  - Examples: "s" in "some," "sh" in "ship."
- **Plosives:**
  - Produced by completely stopping airflow and then releasing it suddenly.
  - Result in a burst of energy in the waveform.
  - Examples: "p" in "put," "t" in "ten."

### 3.3 Co-Articulation
- **Definition:** Overlapping articulatory movements that affect how sounds are produced.
- **Impact:** Neighboring sounds influence each other, making speech sounds context-dependent.

---
## 4. The Sounds of English

### 4.1 Phones and Phonemes
- **Phones:** The distinct speech sounds in a language.
- **Phonemes:** The smallest units of sound that can change meaning.

### 4.2 The ARPAbet Phonetic Alphabet
- **Purpose:** A machine-readable phonetic notation system for American English.
- **Composition:** Approximately 40 phonemes covering vowels, consonants, and diphthongs.
  - **Examples:**
    - **Vowels:** `iy` (as in "bead"), `eh` (as in "bed").
    - **Consonants:** `p, t, k` (voiceless stops), `b, d, g` (voiced stops).
    - **Diphthongs:** `ay` (as in "buy"), `ow` (as in "no").

#### ARPAbet American English Phone Set
![[Pasted image 20241120122028.png]]
### 4.3 Phonetic Transcription Example
- **Sentence:** "This is speech."
- **Transcription:** `th ih s ih z s p iy ch`.
- **Note:** There are no explicit word boundaries in the phonetic sequence.

---
## 5. Consonant and Vowel Classification

### 5.1 Consonant Classification

Consonants are classified based on:

1. **Place of Articulation:** Where the constriction occurs in the vocal tract.

<table style="width:100%; border:1px solid black; border-collapse:collapse;">
  <thead>
    <tr style="background-color:#f2f2f2; font-weight:bold; border:1px solid black;">
      <th style="text-align:left; padding:8px;">Place</th>
      <th style="text-align:left; padding:8px;">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Bilabial</td>
      <td style="padding:8px;">Constriction formed by both lips (e.g., p, b, m).</td>
    </tr>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Labiodental</td>
      <td style="padding:8px;">Constriction formed by the lower lip and the upper teeth (e.g., f, v).</td>
    </tr>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Dental</td>
      <td style="padding:8px;">Constriction formed by the tongue against the upper teeth (e.g., th (thin), dh (then)).</td>
    </tr>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Alveolar</td>
      <td style="padding:8px;">Constriction formed by the tongue against the alveolar ridge (e.g., t, d, s, z, n, l).</td>
    </tr>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Palatal</td>
      <td style="padding:8px;">Constriction formed by the tongue against the hard palate (e.g., sh, zh, ch, jh, y).</td>
    </tr>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Velar</td>
      <td style="padding:8px;">Constriction formed by the tongue against the soft palate (e.g., k, g, ng).</td>
    </tr>
  </tbody>
</table>

2. **Manner of Articulation:** How the airflow is constricted.

<table style="width:100%; border:1px solid black; border-collapse:collapse;">
  <thead>
    <tr style="background-color:#f2f2f2; font-weight:bold; border:1px solid black;">
      <th style="text-align:left; padding:8px;">Manner</th>
      <th style="text-align:left; padding:8px;">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Stops</td>
      <td style="padding:8px;">Complete closure in the vocal tract (e.g., p, t, k)</td>
    </tr>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Fricatives</td>
      <td style="padding:8px;">Narrow constriction causing turbulence (e.g., f, s, sh)</td>
    </tr>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Affricates</td>
      <td style="padding:8px;">Combination of stop and fricative (e.g., ch, jh)</td>
    </tr>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Nasals</td>
      <td style="padding:8px;">Airflow through the nose (e.g., m, n, ng)</td>
    </tr>
    <tr style="border:1px solid black;">
      <td style="padding:8px;">Liquids</td>
      <td style="padding:8px;">Minimal constriction, vowel-like (e.g., l, r)</td>
    </tr>
  </tbody>
</table>


3. **Voicing:** Whether the vocal folds vibrate.
   - **Voiced Consonants:** `b, d, g, z, v, jh`.
   - **Voiceless Consonants:** `p, t, k, s, f, ch`.

The table below shows how each of the consonants in the ARPAbet are classified.
![[Pasted image 20241120122809.png]]
### 5.2 Vowel Classification
Vowels are classified by tongue position and lip rounding:
- **Tongue Height:**
  - High: Tongue raised (e.g., `iy` as in "bead").
  - Mid: Tongue in neutral position (e.g., `eh` as in "bed").
  - Low: Tongue lowered (e.g., `aa` as in "father").
- **Tongue Advancement:**
  - Front: Tongue towards the front of the mouth (e.g., `iy, eh`).
  - Central: Tongue in central position (e.g., `ah` as in "but").
  - Back: Tongue towards the back of the mouth (e.g., `uw` as in "boot").
- **Lip Rounding:**
  - Rounded: Lips rounded (e.g., `uw`).
  - Unrounded: Lips spread (e.g., `iy`).
- **Vowel Quadrilateral**: A diagram representing vowel positions based on tongue height and advancement: 

![[Pasted image 20241120123927.png]]

--- 
## 6. Digital Signals and Analog-to-Digital Conversion (ADC)

### 6.1 The Need for Digital Representation
- **Computational Processing:** Computers process digital signals; thus, speech must be digitized.
- **Advantages:** Easier storage, manipulation, and transmission.

### 6.2 Sampling
- **Process:** Measuring the signal amplitude at discrete time intervals.
- **Sampling Rate:** Number of samples per second (Hz).
- **Nyquist Theorem:** To avoid aliasing, the sampling rate must be at least twice the highest frequency present.
- **Common Rates:**
  - 16 kHz: High-quality speech, capturing frequencies up to 8 kHz.
  - 8 kHz: Telephone quality, capturing frequencies up to 4 kHz.

### 6.3 Quantization
- **Definition:** Converting continuous amplitude values into discrete levels.
- **Bit Depth:** Number of bits per sample, determining quantization levels.
  - 16-bit: 65,536 levels, high precision.
  - 8-bit: 256 levels, lower precision, used in telephony with companding (μ-law or A-law).

### 6.4 Anti-Aliasing Filtering
- **Low-Pass Filter:** Removes frequencies above half the sampling rate before sampling.
- **Purpose:** Prevents high-frequency components from folding into lower frequencies (aliasing).

![[Pasted image 20241120124140.png]]

---
## 7. Speech Analysis and Quasi-Stationarity

### 7.1 Non-Stationarity nature of Speech
- **Challenge:** Speech characteristics change rapidly over time.
- **Impact:** Difficult to analyze and model using traditional stationary signal processing techniques.

### 7.2 Quasi-Stationarity Assumption
- **Definition:** Assuming speech is approximately stationary over short intervals (frames).
- **Frame Duration:** Typically 10–20 milliseconds.
- **Rationale:**
  - **Short Enough:** To capture rapid changes.
  - **Long Enough:** To perform meaningful spectral analysis.

---
## 8. Time-Domain Features of Speech

### 8.1 Visualization
- **Waveform Analysis:** Observing the amplitude of the speech signal over time.
- **Periodic Components:** Indicate voiced sounds with regular patterns.
- **Random Components:** Indicate unvoiced sounds or noise.

### 8.2 Example: The Word "Skills"
- **"s":** Unvoiced fricative, random waveform.
- **"k":** Voiceless plosive, silence followed by a burst.
- **"i":** Vowel, periodic waveform due to voicing.
- **"l":** Liquid, lower amplitude, smoother waveform.
- **"z":** Voiced fricative, combination of periodicity and noise.

![[Pasted image 20241120124345.png]]

---
## 9. Frequency-Domain Analysis

### 9.1 The Fourier Transform (FT)
- **Purpose:** Decomposes a time-domain signal into its constituent frequencies.
- **Components:**
  - **Magnitude Spectrum:** Amplitude of frequency components.
  - **Phase Spectrum:** Phase shift of frequency components.

![[Pasted image 20241120124457.png]]
### 9.2 Significance in Speech Processing
- **Human Perception:** The ear performs a form of spectral analysis.
- **Speech Characteristics:** Many important features (like formants) are better analyzed in the frequency domain.

---
## 10. The Discrete Fourier Transform (DFT)

### 10.1 Mathematical Formulation
Given a discrete-time signal $s(n)$ of length $N$:
$$
S(k) = \sum_{n=0}^{N-1} s(n)e^{-j\frac{2\pi}{N}nk}, \quad k = 0, 1, \dots, N-1
$$
- **$S(k)$:** Complex DFT coefficients representing the frequency content.

**Inverse DFT:**
$$
s(n) = \frac{1}{N} \sum_{k=0}^{N-1} S(k)e^{j\frac{2\pi}{N}nk}
$$

### 10.2 Interpretation
- **Magnitude:** $|S(k)|$ indicates the amplitude of the $k$-th frequency component.
- **Phase:** $\text{arg}(S(k))$ indicates the phase shift.

### 10.3 Practical Considerations
- **Computational Efficiency:** Fast Fourier Transform (FFT) algorithms compute the DFT efficiently ($O(N\log N)$ operations).

---
## 11. Complex Numbers in DFT

### 11.1 Representation
- **Complex Number:** $z = x + jy$.
- **Magnitude:** $|z| = \sqrt{x^2 + y^2}$.
- **Phase:** $\theta = \tan^{-1}\left(\frac{y}{x}\right)$.
- **Euler's Formula:** $e^{j\theta} = \cos\theta + j\sin\theta$.

### 11.2 Application in DFT
- **DFT Coefficients:** Complex numbers representing both amplitude and phase.
- **Analysis:** Enables reconstruction of the time-domain signal from frequency components.

---
## 12. Application of DFT to Speech Signals

### 12.1 Frame-Based Spectral Analysis
- **Process:**
  1. Divide the speech signal into frames (e.g., 10 ms).
  2. Apply a window function to each frame.
  3. Compute the DFT of each windowed frame.
![[Pasted image 20241120124638.png]]
### 12.2 Spectral Characteristics
- **Voiced Sounds:** Show harmonic structure due to periodicity.
- **Unvoiced Sounds:** Display broadband, noise-like spectra.

### 12.3 Example Analysis
- **Vowel Spectrum:** Peaks at formant frequencies.
- **Fricative Spectrum:** Energy concentrated at higher frequencies.

![[Pasted image 20241120124924.png]]

---
## 13. Time vs. Frequency Resolution

### 13.1 Trade-Off
- **Uncertainty Principle:** Cannot simultaneously have high resolution in both time and frequency.
- **Short Frames:**
  - Advantage: Better time resolution.
  - Disadvantage: Poor frequency resolution.
- **Long Frames:**
  - Advantage: Better frequency resolution.
  - Disadvantage: Poor time resolution.

### 13.2 Implications for Speech Processing
- **Balance:** Choose frame length that captures important spectral details without sacrificing temporal responsiveness.
- **Typical Frame Length:** Around 20–25 ms, with a 10 ms shift between frames.

---
## 14. Implicit Periodicity with DFT and Windowing

### 14.1 Implicit Periodicity
- **Assumption:** DFT assumes the analyzed signal is periodic with period equal to the frame length.
- **Issue:** Discontinuities at frame boundaries introduce spectral artifacts (spectral leakage).

### 14.2 Windowing
- **Purpose:** Reduce discontinuities by tapering the signal at the edges.
- **Common Window Functions:**
  - **Hamming Window:**
    $$
    w(n) = 0.54 - 0.46\cos\left(\frac{2\pi n}{N-1}\right)
    $$
  - **Effect:** Smooths the signal, reduces spectral leakage, but slightly broadens spectral peaks.

![[Pasted image 20241120125151.png]]

---
## 15. Spectral Properties of Speech

### 15.1 Formants
- **Definition:** Resonant frequencies of the vocal tract.
- **Role in Vowels:**
  - **First Formant (F1):** Inversely related to vowel height (tongue position).
  - **Second Formant (F2):** Related to tongue advancement (front or back).

### 15.2 Spectral Envelopes
- **Concept:** Smooth curve outlining the spectral peaks.
- **Importance:** Captures the overall shape of the spectrum, crucial for distinguishing phonemes.

### 15.3 Consonant Spectra
- **Nasals:** Low-frequency energy with weak higher formants.
- **Fricatives:** Energy concentrated in high frequencies.
- **Stops:** Characterized by silence (closure) and a burst of energy (release).

![[Pasted image 20241120125230.png]]

---
## 16. Block Processing and Spectrograms

### 16.1 Overlapping Frames
- **Reason:** To capture changes in the speech signal more smoothly.
- **Typical Overlap:** 50% (e.g., 25 ms frames with 10 ms shift).

### 16.2 Spectrograms
- **Definition:** Visual representation of the spectrum over time.
- **Axes:**
  - **Time:** Horizontal axis.
  - **Frequency:** Vertical axis.
  - **Amplitude:** Color intensity or grayscale.

### 16.3 Wideband vs. Narrowband Spectrograms
- **Wideband Spectrograms:**
  - **Shorter Windows:** Better time resolution.
  - **Features:** Individual glottal pulses visible.
- **Narrowband Spectrograms:**
  - **Longer Windows:** Better frequency resolution.
  - **Features:** Harmonic structure of voiced sounds visible.

![[Pasted image 20241120125618.png]]

---
## 17. What is Speech Recognition?

### 17.1 Objective
- **Goal:** Convert speech signals into a sequence of words or other linguistic units.
- **Process:**
  - **Feature Extraction:** Convert raw audio into informative features.
  - **Acoustic Modeling:** Model the relationship between features and phonetic units.
  - **Language Modeling:** Incorporate knowledge about word sequences.

### 17.2 Challenges
- **Variability:**
  - **Inter-Speaker:** Differences between speakers (accent, pitch, speaking rate).
  - **Intra-Speaker:** Variations in the same speaker over time.
  - **Co-Articulation:** Overlapping articulatory movements affect phoneme realization.
  - **Acoustic Conditions:** Background noise, reverberation, channel distortions.

---
## 18. Example Applications of ASR
- **Dictation Systems:** Converting spoken words into text documents.
- **Virtual Assistants:** Voice-controlled systems (e.g., Siri, Alexa, Google Assistant).
- **Transcription Services:** Automatic transcription of meetings, lectures, and voicemails.
- **Call Centers:** Automated customer service using speech recognition.
- **Accessibility:** Assisting individuals with disabilities (e.g., voice control for mobility impairments).
- **Language Learning:** Pronunciation assessment and feedback.

---
## 19. Task Characteristics and Constraints

### 19.1 Task Constraints
- **Vocabulary Size:**
  - **Small Vocabulary:** Limited set of words (e.g., digits).
  - **Large Vocabulary:** Tens of thousands of words (e.g., dictation systems).
- **Grammar and Syntax:**
  - **Constrained:** Specific command phrases.
  - **Unconstrained:** Free-form natural language.
- **Speaking Style:**
  - **Isolated Words:** Pauses between words.
  - **Continuous Speech:** Natural, fluent speech.
- **Speaker Mode:**
  - **Speaker-Dependent:** Trained on a specific user.
  - **Speaker-Independent:** Generalized to multiple users.
  - **Speaker-Adaptive:** Adjusts to new speakers over time.

### 19.2 Environmental Factors
- **Microphone Type:**
  - **Close-Talk:** Microphone near the mouth.
  - **Far-Field:** Distant microphone, more background noise.
- **Noise Conditions:**
  - **Clean:** Quiet environment.
  - **Noisy:** Background sounds, other speakers.

---
## 20. Measuring Word Error Rate (WER) in Continuous Speech Recognition

### 20.1 Definition
$$
\text{WER} = \frac{N_{\text{Substitutions}} + N_{\text{Insertions}} + N_{\text{Deletions}}}{N_{\text{Words in Reference}}} \times 100\%
$$

### 20.2 Error Types
- **Substitution (S):** A word is replaced with an incorrect word.
- **Insertion (I):** An extra word is added.
- **Deletion (D):** A word is omitted.

### 20.3 Alignment Process
- **Dynamic Programming:** Used to align the recognized sequence with the reference transcript.
- **Minimum Edit Distance:** The alignment that results in the fewest errors.

### 20.4 Interpretation
- **WER:** Indicates the overall accuracy of the ASR system.
- **Negative Accuracy:** Possible if the number of errors exceeds the number of words in the reference.

---
## 21. Statistical Speech Recognition

### 21.1 Sequence-to-Sequence Modeling
- **Goal:** Map a sequence of acoustic observations $O$ to a sequence of words $W$.
- **Approach:** Find the word sequence $\hat{W}$ that maximizes the posterior probability $P(W \mid O)$.

### 21.2 Generative Model Framework
$$
\hat{W} = \arg\max_W P(W \mid O) = \arg\max_W P(O \mid W) P(W)
$$
- **Acoustic Model ($P(O \mid W)$):** Probability of observing the acoustic features given the word sequence.
- **Language Model ($P(W)$):** Probability of the word sequence occurring in the language.

---
## 22. Classical HMM-Based Speech Recognition Architecture

### 22.1 Components
- **Feature Extraction:** Converts speech into a sequence of feature vectors.
- **Acoustic Model:** Typically an HMM representing phonemes or sub-word units.
- **Language Model:** Statistical model predicting word sequences (e.g., N-gram models).
- **Pronunciation Lexicon:** Maps words to sequences of phonemes.
- **Decoder/Search Algorithm:** Finds the most probable word sequence given the models.

### 22.2 Process Flow
1. **Signal Processing:** Extract features from the speech signal.
2. **Acoustic Modeling:** Estimate $P(O \mid W)$ using HMMs.
3. **Language Modeling:** Provide $P(W)$ using statistical models.
4. **Search:** Use algorithms like the Viterbi algorithm to find $\hat{W}$.

### 22.3 Adaptation and Adaptivity
- **Speaker Adaptation:** Adjusting models to better fit a particular speaker.
- **Online Adaptation:** Continuously updating models during use.

---
## 23. Feature Extraction

### 23.1 Purpose
- **Dimensionality Reduction:** Convert high-dimensional raw audio into a manageable set of features.
- **Information Preservation:** Retain characteristics important for distinguishing between phonetic units.

### 23.2 Common Techniques
- **Mel-Frequency Cepstral Coefficients (MFCCs):**
  - **Simulate human auditory perception.**
  - **Steps:**
    1. Compute the short-term Fourier Transform.
    2. Apply a mel-scale filter bank.
    3. Take the logarithm of the filter bank energies.
    4. Compute the Discrete Cosine Transform (DCT) to decorrelate features.
- **Perceptual Linear Predictive (PLP) Features:** Incorporate psychoacoustic models.
- **Spectral Features:**
  - **Filter Banks:** Energy in frequency bands.
  - **Delta Features:** Temporal derivatives capturing dynamics.

### 23.3 Frame Rate and Windowing
- **Frame Length:** Typically 25 ms.
- **Frame Shift:** Typically 10 ms.
- **Window Function:** Hamming window to reduce spectral leakage.

---
## 24. Modeling Issues and Time Alignment

### 24.1 High Dimensionality and Variability
- **Output Space:** Large vocabulary sizes increase complexity.
- **Variability:** Need to model variations due to speaker, accent, and environment.

### 24.2 Time Alignment
- **Alignment Need:** Determine which acoustic frames correspond to which phonetic units.
- **HMM Alignment:**
  - States represent phonetic units.
  - Transition probabilities model duration and sequencing.

### 24.3 Sequence Modeling Challenges
- **Long Sequences:** Speech utterances can be lengthy.
- **Variable Lengths:** Different utterances have different lengths.
- **Context Dependency:** Phonemes are affected by neighboring sounds (co-articulation).

---
## 25. Summary
- **Speech Production:** Understanding the human vocal apparatus and the types of sounds produced is fundamental.
- **Signal Processing:** Techniques like the Fourier Transform and windowing are essential for analyzing speech signals.
- **Feature Extraction:** Extracting meaningful features from the speech signal is crucial for ASR performance.
- **Statistical Modeling:** HMMs and language models are core components in traditional ASR systems.
- **Variability Handling:** ASR systems must account for variations in speakers, environments, and speaking styles.
- **Evaluation:** Metrics like WER are used to assess ASR system performance.
- **Advancements:** Modern ASR systems increasingly rely on deep learning and end-to-end architectures for improved performance.

